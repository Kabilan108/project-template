# SLURM Job Arrays

This package contains python modules to help submit jobs to a SLURM cluster 
using job arrays. 

## Usage

There are two main modules in this package: `commands` and `sbatch`.

The `commands` module contains functions to generate commands for running 
python scripts with arguments from yaml files. For example, if you have a
python script called `train.py` that takes arguments from a yaml file called
`config.yaml`, you can use the `generic_yaml_command` function to generate
the command to run the script. 

```python
from slurm.commands import generic_yaml_command

py_path = "./train.py"
yaml_path = "config/config.yaml"
command = generic_yaml_command(py_path, yaml_path)
print(command)
# python ./train.py --config config/config.yaml
```

The `sbatch` module contains functions for submitting job arrays to SLURM
using commands generated by `slurm.commands`. For example, if you have a list
of commands for different experiments, you can use the `submit_array` function
to submit them to SLURM:

```python
from slurm.sbatch import submit_array

root_dir = "/home/user/project"
commands = [
    "python train.py --config config/train-rev1.yaml",
    "python train.py --config config/train-rev2.yaml",
    "python train.py --config config/train-rev3.yaml",
]
node_setting = (
    "--partition=cpu "
    "--nodes=1 " 
    "--ntasks-per-node=1 " 
    "--cpus-per-task=1 "
    "--mem=32G "
    "--time=24:00:00 "
)
job_name = "training"
conda_path = "/home/user/anaconda3/envs/bio"

submit_array(root_dir, commands, node_setting, job_name, conda_path)
# This will create a job_arrays/training.txt file with the commands
# and a scripts/training.sh file with the sbatch script (SLURM settings and
# conda activation) and then run sbatch scripts/training.sh to submit the
# job array to SLURM.
```

The `node_setting` argument is a string that specifies the SLURM parameters for
the job array, such as partition, time limit, CPUs per task, etc. You can find
more information about these parameters from [SLURM documentation](https://slurm.schedmd.com/sbatch.html).

The `job_name` argument is a string that identifies the job array and is used too name the files created by the `submit_array` function.

The `conda_path` argument is an optional string that specifies the path to the 
Anaconda environment that you want to us for running the python scripts. If
you provide this argument, the `submit_array` function will add commands
to load Anaconda and activate the environment in the shell script.

## Example

The `examples/` directory contains a simple python script (`example.py`) that
prints some arguments from a yaml file (`example.yaml`) and a python script
(`example_commands.py`) that generates and submits a job array using the
functions from `slurm.commands` and `slurm.sbatch`. You can run the example
as follows:

```bash
cd examples
python example_commands.py /home/user/examples --partition=cpu --time=10:00 --mem=16G --cpus-per-task=1 example_job /home/user/anaconda3/envs/bio
# This will create a job_arrays/example_job.txt file with three commands
# and a scripts/example_job.sh file with the SLURM settings and conda activation
# and then run sbatch scripts/example_job.sh to submit the job array to SLURM.
```

You can check the status of your job array using the `squeue` command and see
the output of each task in the `output` folder.
